{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNoQc6lcQNvZeW2b5OjjOd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChuchoDC/shape_red/blob/main/ShapeRed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**En caso de utilizar el entorno de google colab, ejecutar la siguiente celda de código**"
      ],
      "metadata": {
        "id": "Hp_N-yMj7doL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAjfVLs-hq-8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yitSxh9H-fZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.losses import MeanAbsoluteError\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "Tf-7c80d-ek8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cargar_datos(csv_file, images_folder, img_size):\n",
        "    datos = pd.read_csv(csv_file)\n",
        "    imagenes = []\n",
        "    landmarks = []\n",
        "\n",
        "    for _, row in datos.iterrows():\n",
        "        img_path = os.path.join(images_folder, row['id'])\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            print(f'Advertencia: No se pudo cargar la imagen {row[\"id\"]}.')\n",
        "            continue\n",
        "\n",
        "        original_size = (img.shape[1], img.shape[0])\n",
        "        img_resized = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n",
        "        img_normalized = img_resized / 255.0\n",
        "        imagenes.append(img_normalized)\n",
        "\n",
        "        coords = row[1:].values.astype(np.float32)\n",
        "        coords[::2] /= original_size[0]\n",
        "        coords[1::2] /= original_size[1]\n",
        "        landmarks.append(coords)\n",
        "\n",
        "    return np.array(imagenes), np.array(landmarks)"
      ],
      "metadata": {
        "id": "MZemToB0-oU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagenes, landmarks = cargar_datos(\n",
        "    csv_file = 'Archivo_csv',\n",
        "    images_folder = 'DirectorioDeImagenes',\n",
        "    img_size = (224, 224)\n",
        ")\n",
        "if imagenes.shape[0] != landmarks.shape[0]:\n",
        "  print(\"Advertencia, no hay el mismo número de Landmarks e imágenes\")\n",
        "\n",
        "print(f\"Imágenes cargadas: {imagenes.shape}\")\n",
        "print(f\"Landmarks cargados: {landmarks.shape}\")"
      ],
      "metadata": {
        "id": "N-QmaAkE-rSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modelo(input_shape, num_landmarks):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (7, 7), activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Conv2D(64, (5, 5), activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        Conv2D(256, (2, 2), activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_landmarks * 2, activation='sigmoid')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "8ahSeqORCHOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (224, 224)\n",
        "input_shape = (img_size[0], img_size[1], 3)\n",
        "num_landmarks =           # Colocar el número de landmarks con los que se cuenta\n",
        "\n",
        "modelo = modeloPropuesto00(input_shape, num_landmarks)\n",
        "modelo.compile(optimizer = Nadam(learning_rate=0.0005), loss=\"mae\")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7)\n",
        "historial = modelo.fit(imagenes, landmarks, epochs=120, batch_size=4, validation_split=0.25, callbacks=[reduce_lr])\n",
        "\n",
        "modelo.save('modeloPropuesto.h5')"
      ],
      "metadata": {
        "id": "1j3Tbu5GCIzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predecir_landmarks(modelo, imagenes, output_csv, img_size=(224, 224)):\n",
        "    predicciones = []\n",
        "\n",
        "    for img_name in os.listdir(imagenes):\n",
        "        img_path = os.path.join(imagenes, img_name)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        if img is None:\n",
        "            print(f'Advertencia: No se pudo cargar la imagen {img_name}.')\n",
        "            continue\n",
        "\n",
        "        original_size = (img.shape[1], img.shape[0])\n",
        "        img = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n",
        "        img = img / 255.0\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "\n",
        "        pred = modelo.predict(img, verbose=0)[0]\n",
        "\n",
        "        pred[::2] *= original_size[0]\n",
        "        pred[1::2] *= original_size[1]\n",
        "\n",
        "        predicciones.append([img_name] + pred.tolist())\n",
        "\n",
        "    columnas = ['id'] + [f'X{i}' for i in range(len(pred) // 2)] + [f'Y{i}' for i in range(len(pred) // 2)]\n",
        "    resultados = pd.DataFrame(predicciones, columns=columnas)\n",
        "    resultados.to_csv(output_csv, index=False)\n",
        "    print(f'Resultados guardados en {output_csv}')"
      ],
      "metadata": {
        "id": "8ogO06o5ChYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carpeta_imagenes = '/content/drive/MyDrive/NeuralNetworks/Proyecto_Morphometry_NN/PecesSimilares'\n",
        "output_csv = 'CNN.csv'\n",
        "\n",
        "modelo = load_model('modeloPropuesto.h5', custom_objects={'mae': MeanAbsoluteError()})\n",
        "predecir_landmarks(modelo, carpeta_imagenes, output_csv)"
      ],
      "metadata": {
        "id": "Vy6tl_WACkVj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}